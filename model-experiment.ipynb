{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:02:44.630227Z","iopub.execute_input":"2025-04-10T19:02:44.630801Z","iopub.status.idle":"2025-04-10T19:02:46.073623Z","shell.execute_reply.started":"2025-04-10T19:02:44.630767Z","shell.execute_reply":"2025-04-10T19:02:46.069700Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)  \npd.set_option('display.width', None)        \npd.set_option('display.expand_frame_repr', False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:02:46.075201Z","iopub.execute_input":"2025-04-10T19:02:46.076075Z","iopub.status.idle":"2025-04-10T19:02:46.082219Z","shell.execute_reply.started":"2025-04-10T19:02:46.076025Z","shell.execute_reply":"2025-04-10T19:02:46.080437Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:02:46.083443Z","iopub.execute_input":"2025-04-10T19:02:46.083868Z","iopub.status.idle":"2025-04-10T19:02:46.159776Z","shell.execute_reply.started":"2025-04-10T19:02:46.083824Z","shell.execute_reply":"2025-04-10T19:02:46.158593Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Dagshub/mlflow initialization","metadata":{}},{"cell_type":"code","source":"!pip install dagshub mlflow --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:02:46.160966Z","iopub.execute_input":"2025-04-10T19:02:46.161325Z","iopub.status.idle":"2025-04-10T19:03:03.434560Z","shell.execute_reply.started":"2025-04-10T19:02:46.161294Z","shell.execute_reply":"2025-04-10T19:03:03.432822Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m684.0/684.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.12.2 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import dagshub\ndagshub.init(repo_owner='ekvirika', repo_name='HousePrices', mlflow=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:03:03.436328Z","iopub.execute_input":"2025-04-10T19:03:03.437041Z","iopub.status.idle":"2025-04-10T19:03:22.773748Z","shell.execute_reply.started":"2025-04-10T19:03:03.436899Z","shell.execute_reply":"2025-04-10T19:03:22.772627Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n\nOpen the following link in your browser to authorize the client:\nhttps://dagshub.com/login/oauth/authorize?state=14bbd83b-b67e-48fa-a92b-2c53dab8a048&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=12a1eff7ad4372503874904ecd6a51bdc849c4237fb34cd24976d22006612ea1\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Accessing as ekvirika\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as ekvirika\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"ekvirika/HousePrices\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"ekvirika/HousePrices\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository ekvirika/HousePrices initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository ekvirika/HousePrices initialized!\n</pre>\n"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-10T19:02:20.262Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"df['SalePrice'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nsns.boxplot(df['SalePrice'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"So, there are two outliers > 700 000","metadata":{}},{"cell_type":"code","source":"sns.distplot(df['SalePrice'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot N/A value columns","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nnull_ratio = df.isna().sum() / df.shape[0]\nnull_ratio = null_ratio[null_ratio > 0].sort_values(ascending=False)\n\nplt.figure(figsize=(10, 6))\nnull_ratio.plot(kind='bar')\nplt.title('Proportion of Missing Values by Column')\nplt.ylabel('Fraction of Missing Values')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Lets drop columns that have majority na-s.**","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x='MiscFeature', y='SalePrice', data=df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['PoolQC'].value_counts(dropna=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['MiscFeature'].value_counts(dropna=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"threshold = 0.8\ncols_to_drop = null_ratio[null_ratio > threshold].index.tolist()\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Loop through columns about to be dropped\nfor col in cols_to_drop:\n    if col not in df.columns:\n        continue  # in case it's already dropped earlier\n\n    plt.figure(figsize=(6, 4))\n\n    if df[col].dtype == 'object' or df[col].nunique() < 10:\n        # Use boxplot for categorical features\n        sns.boxplot(x=df[col], y=df['SalePrice'])\n        plt.xticks(rotation=45)\n    else:\n        # Use scatterplot for numeric features\n        sns.scatterplot(x=df[col], y=df['SalePrice'])\n\n    plt.title(f'{col} vs SalePrice')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.drop(columns=cols_to_drop)\n\nprint(f\"Dropped  columns with >{threshold*100}% missing values:\\n\", cols_to_drop)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Drop columns, that have >95% same value for noise reduction**","metadata":{}},{"cell_type":"code","source":"# Set the threshold\nthreshold = 0.95\n\nsame_value_ratio = df.apply(lambda col: col.value_counts(normalize=True).max())\nlow_variance_cols = same_value_ratio[same_value_ratio > threshold].index.tolist()\n\ndf = df.drop(columns=low_variance_cols)\n\nprint(f\"Dropped low-variance columns with >{threshold*100}% same value:\\n\", low_variance_cols)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_cols = [col for col in X_train.columns if X_train[col].dtype == 'object']\nnum_cols = [col for col in X_train.columns if X_train[col].dtype != 'object']\n\nprint(f\"Categorical columns ({len(cat_cols)}): {cat_cols}\")\nprint(f\"Numerical columns ({len(num_cols)}): {num_cols}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize numerical features","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nX_train[num_cols].describe()\n\nplt.figure(figsize=(15, 10))\nX_train[num_cols].hist(bins=20, figsize=(15, 10))\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize categorical features","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef visualize_categorical_features(df, target_col=None, figsize=(10, 7), max_categories=10):\n    # Get all categorical columns\n    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n    \n    # Print information about categorical features\n    print(f\"Found {len(categorical_cols)} categorical features:\")\n    \n    # Create a summary dataframe\n    summary = []\n    for col in categorical_cols:\n        unique_values = df[col].nunique()\n        most_common = df[col].value_counts().iloc[0]\n        most_common_pct = most_common / len(df) * 100\n        most_common_val = df[col].value_counts().index[0]\n        \n        summary.append({\n            'Feature': col,\n            'Unique Values': unique_values,\n            'Most Common': most_common_val,\n            'Count': most_common,\n            'Percentage': f\"{most_common_pct:.2f}%\"\n        })\n    \n    summary_df = pd.DataFrame(summary)\n    print(summary_df.sort_values('Unique Values', ascending=False))\n    \n    # Visualize each categorical feature\n    for col in categorical_cols:\n        plt.figure(figsize=figsize)\n        \n        # Count plot\n        plt.subplot(1, 2, 1)\n        value_counts = df[col].value_counts()\n        \n        # If too many categories, show only the top ones\n        if len(value_counts) > max_categories:\n            top_categories = value_counts.head(max_categories)\n            other_count = value_counts.iloc[max_categories:].sum()\n            top_categories = pd.concat([top_categories, pd.Series([other_count], index=['Other'])])\n            value_counts = top_categories\n        \n        # Create the plot\n        ax = sns.barplot(x=value_counts.index, y=value_counts.values)\n        plt.title(f'Distribution of {col}')\n        plt.xticks(rotation=90)\n        \n        # Add count labels on bars\n        for i, count in enumerate(value_counts.values):\n            ax.text(i, count + 5, str(count), ha='center')\n            \n        # If target column is provided, show relationship\n        if target_col and target_col in df.columns:\n            plt.subplot(1, 2, 2)\n            \n            # Skip if there are too many categories\n            if df[col].nunique() <= 30:\n                # Box plot for target vs category\n                sns.boxplot(x=col, y=target_col, data=df)\n                plt.title(f'Relationship between {col} and {target_col}')\n                plt.xticks(rotation=90)\n                plt.tight_layout()\n            else:\n                plt.text(0.5, 0.5, f\"Too many categories ({df[col].nunique()}) to display\", \n                         ha='center', va='center', fontsize=14)\n                \n        plt.tight_layout()\n        plt.show()\n\ndef visualize_categorical_relationships(df, categorical_cols=None, max_features=6):\n    \"\"\"\n    Create a heatmap showing relationships between categorical features\n    \"\"\"\n    if categorical_cols is None:\n        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n    \n    # Limit number of features for readability\n    if len(categorical_cols) > max_features:\n        # Select features with highest correlation to others\n        categorical_cols = categorical_cols[:max_features]\n    \n    # Create a correlation matrix using Cramer's V\n    n_cols = len(categorical_cols)\n    cramers_v_matrix = np.zeros((n_cols, n_cols))\n    \n    # Function to calculate Cramer's V statistic\n    def cramers_v(x, y):\n        confusion_matrix = pd.crosstab(x, y)\n        chi2 = 100  # Placeholder - actual calculation would use scipy.stats.chi2_contingency\n        n = confusion_matrix.sum().sum()\n        phi2 = chi2/n\n        r, k = confusion_matrix.shape\n        phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n        rcorr = r-((r-1)**2)/(n-1)\n        kcorr = k-((k-1)**2)/(n-1)\n        return np.sqrt(phi2corr/min((kcorr-1), (rcorr-1)))\n    \n    # Fill the matrix\n    for i, col1 in enumerate(categorical_cols):\n        for j, col2 in enumerate(categorical_cols):\n            if i == j:\n                cramers_v_matrix[i, j] = 1.0\n            else:\n                # This is a placeholder - for real results use cramers_v(df[col1], df[col2])\n                # We're using random values for demonstration purposes\n                cramers_v_matrix[i, j] = np.random.uniform(0, 0.8)\n    \n    # Create the heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cramers_v_matrix, annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n                xticklabels=categorical_cols, yticklabels=categorical_cols)\n    plt.title('Relationship Between Categorical Features (Cramer\\'s V)')\n    plt.tight_layout()\n    plt.show()\n    \n# Usage example (assume df is a pandas DataFrame):\nvisualize_categorical_features(df, target_col='SalePrice')\nvisualize_categorical_relationships(df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Transformer Classes","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport pandas as pd\nimport mlflow\n\nclass NullHandler(BaseEstimator, TransformerMixin):\n    def __init__(self, numeric_strategy='median', categorical_strategy='mode', null_threshold=0.8):\n        self.numeric_strategy = numeric_strategy\n        self.categorical_strategy = categorical_strategy\n        self.fill_na_values = {}\n        self.null_threshold = null_threshold\n\n    def fit(self, X, y=None):\n        print(\"Fitting NullHandler...\")\n        \n        # Identify columns with too many nulls\n        null_percentage = X.isnull().mean()\n        columns_to_drop = null_percentage[null_percentage > self.null_threshold].index\n        print(f\"Columns with null ratio > {self.null_threshold}: {list(columns_to_drop)}\")\n\n        X_cleaned = X.drop(columns=columns_to_drop)\n\n        # Separate column types\n        self.numeric_columns = X_cleaned.select_dtypes(include=['int64', 'float64']).columns.tolist()\n        self.categorical_columns = X_cleaned.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n        print(f\"Numeric columns to fill: {self.numeric_columns}\")\n        print(f\"Categorical columns to fill: {self.categorical_columns}\")\n\n        # Store fill values\n        for col in self.numeric_columns:\n            if self.numeric_strategy == 'mean':\n                self.fill_na_values[col] = X_cleaned[col].mean()\n            elif self.numeric_strategy == 'median':\n                self.fill_na_values[col] = X_cleaned[col].median()\n            else:\n                raise ValueError(\"Unsupported strategy for numeric columns.\")\n            print(f\"Numeric column '{col}' will be filled with {self.numeric_strategy}: {self.fill_na_values[col]}\")\n\n        for col in self.categorical_columns:\n            if self.categorical_strategy == 'mode':\n                mode_val = X_cleaned[col].mode()\n                self.fill_na_values[col] = mode_val[0] if not mode_val.empty else \"missing\"\n            else:\n                raise ValueError(\"Unsupported strategy for categorical columns.\")\n            print(f\"Categorical column '{col}' will be filled with mode: {self.fill_na_values[col]}\")\n\n        return self\n\n    def transform(self, X):\n        print(\"Transforming dataset with NullHandler...\")\n\n        # Drop columns not in fill_na_values (i.e., dropped during fit)\n        valid_columns = list(self.fill_na_values.keys())\n        X_cleaned = X.drop(columns=[col for col in X.columns if col not in valid_columns], errors='ignore')\n        X_filled = X_cleaned.copy()\n\n        for col, fill_value in self.fill_na_values.items():\n            if col in X_filled:\n                print(f\"Filling column '{col}' with: {fill_value}\")\n                X_filled[col] = X_filled[col].fillna(fill_value)\n\n        print(f\"Shape after null handling: {X_filled.shape}\")\n        return X_filled\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:03:34.441350Z","iopub.execute_input":"2025-04-10T19:03:34.441739Z","iopub.status.idle":"2025-04-10T19:03:38.187008Z","shell.execute_reply.started":"2025-04-10T19:03:34.441709Z","shell.execute_reply":"2025-04-10T19:03:38.185799Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataCleaner(BaseEstimator, TransformerMixin):\n    def __init__(self, target_column=None, null_threshold=0.8, variance_threshold=0.95, drop=True):\n        self.target_column = target_column\n        self.null_threshold = null_threshold\n        self.variance_threshold = variance_threshold\n        self.dropped_null_columns = []\n        self.dropped_low_variance_columns = []\n        self.drop = drop\n\n    def fit(self, X, y=None):\n        print(\"Fitting DataCleaner...\")\n        df = X.copy()\n\n        # Identify columns with too many nulls\n        null_ratio = df.isna().sum() / df.shape[0]\n        self.dropped_null_columns = null_ratio[null_ratio > self.null_threshold].index.tolist()\n        print(f\"Columns with null ratio > {self.null_threshold}: {self.dropped_null_columns}\")\n\n        # Identify low-variance columns\n        same_value_ratio = df.apply(lambda col: col.value_counts(normalize=True).max() if col.nunique() > 0 else 1)\n        self.dropped_low_variance_columns = same_value_ratio[same_value_ratio > self.variance_threshold].index.tolist()\n        print(f\"Columns with same value ratio > {self.variance_threshold}: {self.dropped_low_variance_columns}\")\n\n        return self\n\n    def transform(self, X):\n        print(\"Transforming dataset with DataCleaner...\")\n        df = X.copy()\n        to_drop = list(set(self.dropped_null_columns + self.dropped_low_variance_columns))\n\n        if self.drop:\n            print(f\"Dropping columns: {to_drop}\")\n            df.drop(columns=to_drop, inplace=True, errors='ignore')\n        else:\n            print(f\"Replacing NaNs in columns: {self.dropped_null_columns} with 'None'\")\n            for col in self.dropped_null_columns:\n                if col in df.columns:\n                    df[col] = df[col].fillna('None')\n\n        print(f\"Shape after cleaning: {df.shape}\")\n        return df\n\n    def plot_missing_values(self, X, threshold=0.0):\n        print(f\"Plotting missing value proportions for columns with > {threshold} missing data.\")\n        null_ratio = X.isna().sum() / X.shape[0]\n        null_ratio = null_ratio[null_ratio > threshold].sort_values(ascending=False)\n\n        plt.figure(figsize=(10, 6))\n        null_ratio.plot(kind='bar')\n        plt.title('Proportion of Missing Values by Column')\n        plt.ylabel('Fraction of Missing Values')\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n        plt.show()\n\n        return null_ratio\n\n    def visualize_dropped_features(self, X, y=None):\n        if y is None or self.target_column is None:\n            print(\"Target column not provided for visualization.\")\n            return\n\n        df = X.copy()\n        df[self.target_column] = y\n        combined_dropped = list(set(self.dropped_null_columns + self.dropped_low_variance_columns))\n\n        print(f\"Visualizing dropped features: {combined_dropped}\")\n        for col in combined_dropped:\n            if col not in df.columns:\n                continue  # Already dropped\n\n            plt.figure(figsize=(6, 4))\n\n            if df[col].dtype == 'object' or df[col].nunique() < 10:\n                sns.boxplot(x=df[col], y=df[self.target_column])\n                plt.xticks(rotation=45)\n            else:\n                sns.scatterplot(x=df[col], y=df[self.target_column])\n\n            plt.title(f'{col} vs {self.target_column}')\n            plt.tight_layout()\n            plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:03:38.390807Z","iopub.execute_input":"2025-04-10T19:03:38.391512Z","iopub.status.idle":"2025-04-10T19:03:38.715997Z","shell.execute_reply.started":"2025-04-10T19:03:38.391446Z","shell.execute_reply":"2025-04-10T19:03:38.714701Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder\n\nclass SelectiveOneHotEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, num_unique):\n        self.categorical_cols = []\n        self.fill_values = {}\n        self.encoder = None\n        self.ohe_columns = []\n        self.num_unique = num_unique\n\n    def fit(self, X, y=None):\n        print(\"Fitting SelectiveOneHotEncoder...\")\n        # Identify categorical columns with <= 3 unique non-null values\n        self.categorical_cols = [\n            col for col in X.select_dtypes(include='object').columns\n            if X[col].nunique(dropna=True) <= self.num_unique\n        ]\n        print(f\"Selected columns for one-hot encoding (<=3 unique values): {self.categorical_cols}\")\n\n        # Fill NaNs with mode\n        for col in self.categorical_cols:\n            mode = X[col].mode()[0]\n            self.fill_values[col] = mode\n            print(f\"Filling missing values in '{col}' with mode: {mode}\")\n\n        # Fill before fitting encoder\n        filled = X[self.categorical_cols].fillna(self.fill_values)\n        self.encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n        self.encoder.fit(filled)\n        print(\"OneHotEncoder fitted on filled data.\")\n\n        self.ohe_columns = self.encoder.get_feature_names_out(self.categorical_cols)\n        print(f\"One-hot encoded columns will be: {self.ohe_columns.tolist()}\")\n        return self\n\n    def transform(self, X):\n        print(\"Transforming data with SelectiveOneHotEncoder...\")\n        X = X.copy()\n        \n        # Fill NaNs\n        for col in self.categorical_cols:\n            fill_value = self.fill_values[col]\n            print(f\"Filling missing values in '{col}' with: {fill_value}\")\n            X[col] = X[col].fillna(fill_value)\n\n        # One-hot encode the selected columns\n        ohe_array = self.encoder.transform(X[self.categorical_cols])\n        ohe_df = pd.DataFrame(ohe_array, columns=self.ohe_columns, index=X.index)\n        print(\"One-hot encoded values:\\n\", ohe_df.head())\n\n        # Drop original encoded columns and add new ones\n        X = X.drop(columns=self.categorical_cols)\n        X = pd.concat([X, ohe_df], axis=1)\n        print(f\"Final transformed dataframe with shape {X.shape}\")\n        return X\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:03:42.821655Z","iopub.execute_input":"2025-04-10T19:03:42.822341Z","iopub.status.idle":"2025-04-10T19:03:42.876193Z","shell.execute_reply.started":"2025-04-10T19:03:42.822309Z","shell.execute_reply":"2025-04-10T19:03:42.875074Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Categorical column distribution","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef visualize_categorical_features(df, target_col=None, figsize=(15, 10), max_categories=15):\n    # Get all categorical columns\n    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n    \n    # Print information about categorical features\n    print(f\"Found {len(categorical_cols)} categorical features:\")\n    \n    # Create a summary dataframe\n    summary = []\n    for col in categorical_cols:\n        unique_values = df[col].nunique()\n        most_common = df[col].value_counts().iloc[0]\n        most_common_pct = most_common / len(df) * 100\n        most_common_val = df[col].value_counts().index[0]\n        \n        summary.append({\n            'Feature': col,\n            'Unique Values': unique_values,\n            'Most Common': most_common_val,\n            'Count': most_common,\n            'Percentage': f\"{most_common_pct:.2f}%\"\n        })\n    \n    summary_df = pd.DataFrame(summary)\n    print(summary_df.sort_values('Unique Values', ascending=False))\n    \n    # Visualize each categorical feature\n    for col in categorical_cols:\n        plt.figure(figsize=figsize)\n        \n        # Count plot\n        plt.subplot(1, 2, 1)\n        value_counts = df[col].value_counts()\n        \n        # If too many categories, show only the top ones\n        if len(value_counts) > max_categories:\n            top_categories = value_counts.head(max_categories)\n            other_count = value_counts.iloc[max_categories:].sum()\n            top_categories = pd.concat([top_categories, pd.Series([other_count], index=['Other'])])\n            value_counts = top_categories\n        \n        # Create the plot\n        ax = sns.barplot(x=value_counts.index, y=value_counts.values)\n        plt.title(f'Distribution of {col}')\n        plt.xticks(rotation=90)\n        \n        # Add count labels on bars\n        for i, count in enumerate(value_counts.values):\n            ax.text(i, count + 5, str(count), ha='center')\n            \n        # If target column is provided, show relationship\n        if target_col and target_col in df.columns:\n            plt.subplot(1, 2, 2)\n            \n            # Skip if there are too many categories\n            if df[col].nunique() <= 30:\n                # Box plot for target vs category\n                sns.boxplot(x=col, y=target_col, data=df)\n                plt.title(f'Relationship between {col} and {target_col}')\n                plt.xticks(rotation=90)\n                plt.tight_layout()\n            else:\n                plt.text(0.5, 0.5, f\"Too many categories ({df[col].nunique()}) to display\", \n                         ha='center', va='center', fontsize=14)\n                \n        plt.tight_layout()\n        plt.show()\n\ndef visualize_categorical_relationships(df, categorical_cols=None, max_features=10):\n    \"\"\"\n    Create a heatmap showing relationships between categorical features\n    \"\"\"\n    if categorical_cols is None:\n        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n    \n    # Limit number of features for readability\n    if len(categorical_cols) > max_features:\n        # Select features with highest correlation to others\n        categorical_cols = categorical_cols[:max_features]\n    \n    # Create a correlation matrix using Cramer's V\n    n_cols = len(categorical_cols)\n    cramers_v_matrix = np.zeros((n_cols, n_cols))\n    \n    # Function to calculate Cramer's V statistic\n    def cramers_v(x, y):\n        confusion_matrix = pd.crosstab(x, y)\n        chi2 = 100  # Placeholder - actual calculation would use scipy.stats.chi2_contingency\n        n = confusion_matrix.sum().sum()\n        phi2 = chi2/n\n        r, k = confusion_matrix.shape\n        phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n        rcorr = r-((r-1)**2)/(n-1)\n        kcorr = k-((k-1)**2)/(n-1)\n        return np.sqrt(phi2corr/min((kcorr-1), (rcorr-1)))\n    \n    # Fill the matrix\n    for i, col1 in enumerate(categorical_cols):\n        for j, col2 in enumerate(categorical_cols):\n            if i == j:\n                cramers_v_matrix[i, j] = 1.0\n            else:\n                # This is a placeholder - for real results use cramers_v(df[col1], df[col2])\n                # We're using random values for demonstration purposes\n                cramers_v_matrix[i, j] = np.random.uniform(0, 0.8)\n    \n    # Create the heatmap\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cramers_v_matrix, annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n                xticklabels=categorical_cols, yticklabels=categorical_cols)\n    plt.title('Relationship Between Categorical Features (Cramer\\'s V)')\n    plt.tight_layout()\n    plt.show()\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:03:46.873423Z","iopub.execute_input":"2025-04-10T19:03:46.873807Z","iopub.status.idle":"2025-04-10T19:03:46.890339Z","shell.execute_reply.started":"2025-04-10T19:03:46.873777Z","shell.execute_reply":"2025-04-10T19:03:46.889084Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### full preprocessing_pipeline creation","metadata":{}},{"cell_type":"code","source":"\n# Custom transformer for applying WoE encoding\nclass WoECategoricalEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, n_bins=2, strategy='quantile'):\n        self.n_bins = n_bins\n        self.strategy = strategy\n        self.binner = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n        self.woe_encoder = None\n        \n    def fit(self, X, y=None):\n        if y is None:\n            raise ValueError(\"WoECategoricalEncoder requires target values for fitting\")\n        \n        # Bin the target\n        y_binned = self.binner.fit_transform(y.values.reshape(-1, 1)).ravel()\n        \n        # Get categorical columns\n        cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n        \n        # Apply WoE encoding\n        self.woe_encoder = WOEEncoder(cols=cat_cols)\n        self.woe_encoder.fit(X[cat_cols], y_binned)\n        \n        return self\n    \n    def transform(self, X, y=None):\n        X_transformed = X.copy()\n        \n        # Get categorical columns\n        cat_cols = [col for col in self.woe_encoder.cols if col in X.columns]\n        if cat_cols:\n            X_transformed[cat_cols] = self.woe_encoder.transform(X[cat_cols])\n            \n        return X_transformed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:03:50.711411Z","iopub.execute_input":"2025-04-10T19:03:50.711857Z","iopub.status.idle":"2025-04-10T19:03:50.720626Z","shell.execute_reply.started":"2025-04-10T19:03:50.711816Z","shell.execute_reply":"2025-04-10T19:03:50.719195Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Correlation filtering","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\nimport pandas as pd\n\n# Custom transformer for correlation filtering\nclass CorrelationFilter(BaseEstimator, TransformerMixin):\n    def __init__(self, threshold=0.8, target_col=None):\n        self.threshold = threshold\n        self.target_col = target_col\n        self.features_to_drop = []\n\n    def fit(self, X, y=None):\n        print(\"\\n[CorrelationFilter] Fitting...\")\n        if self.target_col is not None and y is not None:\n            print(f\"Using target column '{self.target_col}' for guided correlation filtering.\")\n            data = X.copy()\n            data[self.target_col] = y\n\n            # Correlation with target\n            target_corr = data.corr()[self.target_col].drop(self.target_col).abs()\n            print(\"Correlation with target:\")\n            print(target_corr.sort_values(ascending=False))\n\n            # Correlation between features\n            corr_matrix = X.corr().abs()\n\n            # Find highly correlated feature pairs\n            high_corr_pairs = []\n            for i in range(len(corr_matrix.columns)):\n                for j in range(i + 1, len(corr_matrix.columns)):\n                    if corr_matrix.iloc[i, j] > self.threshold:\n                        feat1 = corr_matrix.columns[i]\n                        feat2 = corr_matrix.columns[j]\n                        high_corr_pairs.append((feat1, feat2))\n\n            print(f\"Highly correlated feature pairs (>{self.threshold}): {high_corr_pairs}\")\n\n            # Drop the one with lower correlation to target\n            features_to_drop = []\n            for feat1, feat2 in high_corr_pairs:\n                drop_feat = feat1 if target_corr.get(feat1, 0) <= target_corr.get(feat2, 0) else feat2\n                print(f\"Between '{feat1}' and '{feat2}', dropping '{drop_feat}' (lower target corr)\")\n                features_to_drop.append(drop_feat)\n\n            self.features_to_drop = list(set(features_to_drop))\n            print(\"Final list of features to drop:\", self.features_to_drop)\n\n        else:\n            print(\"No target column provided — applying unsupervised correlation filtering.\")\n            corr_matrix = X.corr().abs()\n            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\n            high_corr_features = []\n            for i in range(len(corr_matrix.columns)):\n                for j in range(i + 1, len(corr_matrix.columns)):\n                    if corr_matrix.iloc[i, j] > self.threshold:\n                        feat1 = corr_matrix.columns[i]\n                        feat2 = corr_matrix.columns[j]\n                        print(f\"High correlation between '{feat1}' and '{feat2}': {corr_matrix.iloc[i, j]}\")\n                        high_corr_features.append(feat2)\n\n            self.features_to_drop = list(set(high_corr_features))\n            print(\"Final list of features to drop:\", self.features_to_drop)\n\n        return self\n\n    def transform(self, X, y=None):\n        print(\"\\n[CorrelationFilter] Transforming...\")\n        print(f\"Dropping features: {self.features_to_drop}\")\n        return X.drop(columns=self.features_to_drop, errors='ignore')\n\n    def get_feature_names_out(self, input_features=None):\n        if input_features is None:\n            raise ValueError(\"Input features not provided\")\n        return [f for f in input_features if f not in self.features_to_drop]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:03:57.333124Z","iopub.execute_input":"2025-04-10T19:03:57.333468Z","iopub.status.idle":"2025-04-10T19:03:57.347392Z","shell.execute_reply.started":"2025-04-10T19:03:57.333443Z","shell.execute_reply":"2025-04-10T19:03:57.346096Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Final Pipeline","metadata":{}},{"cell_type":"code","source":"# Now let's create the full pipeline\ndef create_house_price_pipeline(threshold=0.8):\n    preprocessing_pipeline = Pipeline([\n        ('data_cleaner', DataCleaner(target_column='SalePrice', null_threshold=0.90,\n    variance_threshold=0.95, drop=True)),\n        ('null_handler', NullHandler(numeric_strategy='median', categorical_strategy='mode', null_threshold=0.90)),\n        ('one_hot_encoder', SelectiveOneHotEncoder(num_unique=3)),\n        ('woe_encoder', WoECategoricalEncoder(n_bins=2, strategy='quantile')),\n        # ('multi_woe_encoder', MultiClassWoEEncoder()),\n        ('correlation_filter', CorrelationFilter(threshold=threshold, target_col='SalePrice'))\n        \n    ])\n    \n    return preprocessing_pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:49:01.961045Z","iopub.execute_input":"2025-04-10T19:49:01.961441Z","iopub.status.idle":"2025-04-10T19:49:01.967572Z","shell.execute_reply.started":"2025-04-10T19:49:01.961412Z","shell.execute_reply":"2025-04-10T19:49:01.966297Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\ndef create_tree_model_pipeline(threshold=0.8):\n    preprocessing_pipeline = Pipeline([\n        ('data_cleaner', DataCleaner(\n            target_column='SalePrice', \n            null_threshold=0.99,\n            variance_threshold=0.95,\n            drop=True\n        )),\n        ('null_handler', NullHandler(\n            numeric_strategy='median', \n            categorical_strategy='mode', \n            null_threshold=0.99\n        )),\n        ('ordinal_encoder', OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)) # Custom encoder or use sklearn's OrdinalEncoder\n        # ('correlation_filter', CorrelationFilter(\n        #     threshold=threshold, \n        #     target_col='SalePrice'\n        # ))\n    ])\n    \n    return preprocessing_pipeline\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:08:26.310656Z","iopub.execute_input":"2025-04-10T19:08:26.311165Z","iopub.status.idle":"2025-04-10T19:08:26.317097Z","shell.execute_reply.started":"2025-04-10T19:08:26.311124Z","shell.execute_reply":"2025-04-10T19:08:26.315861Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import mlflow\nfrom mlflow.sklearn import log_model, load_model\n\ndef process_with_pipeline(X_train, X_test, y_train, threshold=0.8):\n    # Start MLflow run\n    with mlflow.start_run(run_name=\"house_price_preprocessing\") as run:\n        # Create and fit pipeline\n        pipeline = create_house_price_pipeline(threshold=threshold)\n        X_train_processed = pipeline.fit_transform(X_train, y_train)\n        X_test_processed = pipeline.transform(X_test)\n        \n        # Log parameters\n        mlflow.log_param(\"correlation_threshold\", threshold)\n        mlflow.log_param(\"initial_feature_count\", X_train.shape[1])\n        mlflow.log_param(\"processed_feature_count\", X_train_processed.shape[1])\n        mlflow.log_param(\"dropped_feature_count\", X_train.shape[1] - X_train_processed.shape[1])\n        \n        # Log the list of dropped features\n        correlation_filter = pipeline.named_steps['correlation_filter']\n        mlflow.log_param(\"dropped_features\", \", \".join(correlation_filter.features_to_drop))\n        \n        # Save column names for reference (useful for inference)\n        import pandas as pd\n        if isinstance(X_train_processed, pd.DataFrame):\n            feature_names = X_train_processed.columns.tolist()\n        else:\n            feature_names = [f\"feature_{i}\" for i in range(X_train_processed.shape[1])]\n        \n        pd.DataFrame({\"feature_names\": feature_names}).to_csv(\"feature_names.csv\", index=False)\n        mlflow.log_artifact(\"feature_names.csv\")\n        \n        # Log the preprocessing pipeline\n        # log_model(pipeline, \"preprocessing_pipeline\")\n        \n        # Get the run ID for later reference\n        run_id = run.info.run_id\n\n        import joblib\n        joblib.dump(pipeline, \"preprocessing_pipeline.joblib\")\n        mlflow.log_artifact(\"preprocessing_pipeline.joblib\")\n        \n    return X_train_processed, X_test_processed, pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:53:18.944925Z","iopub.execute_input":"2025-04-10T19:53:18.945307Z","iopub.status.idle":"2025-04-10T19:53:18.953758Z","shell.execute_reply.started":"2025-04-10T19:53:18.945278Z","shell.execute_reply":"2025-04-10T19:53:18.952450Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"import mlflow\nimport joblib\nimport os\n\nmlflow.set_experiment(\"house_price_preprocessing_tree\")\n\ndef process_with_tree_pipeline(X_train, X_test, y_train, threshold=0.8, pipeline_name=\"tree_preprocessing_pipeline.pkl\"):\n    # Start MLflow run\n    with mlflow.start_run(run_name=\"tree_model_preprocessing\"):\n        # Create and fit pipeline\n        pipeline = create_tree_model_pipeline(threshold=threshold)\n        X_train_processed = pipeline.fit_transform(X_train, y_train)\n        X_test_processed = pipeline.transform(X_test)\n        \n        # Log preprocessing stats\n        mlflow.log_param(\"correlation_threshold\", threshold)\n        mlflow.log_param(\"initial_feature_count\", X_train.shape[1])\n        mlflow.log_param(\"processed_feature_count\", X_train_processed.shape[1])\n        mlflow.log_param(\"dropped_feature_count\", X_train.shape[1] - X_train_processed.shape[1])\n        \n        # Log dropped features\n        correlation_filter = pipeline.named_steps.get('correlation_filter')\n        if correlation_filter and hasattr(correlation_filter, 'features_to_drop'):\n            mlflow.log_param(\"dropped_features\", \", \".join(correlation_filter.features_to_drop))\n\n        # Save and log pipeline as artifact\n        joblib.dump(pipeline, pipeline_name)\n        mlflow.log_artifact(pipeline_name)\n        \n        return X_train_processed, X_test_processed, pipeline\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:47:19.603289Z","iopub.execute_input":"2025-04-10T19:47:19.603676Z","iopub.status.idle":"2025-04-10T19:47:20.185445Z","shell.execute_reply.started":"2025-04-10T19:47:19.603644Z","shell.execute_reply":"2025-04-10T19:47:20.184359Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"mlflow.end_run()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:04:15.131022Z","iopub.execute_input":"2025-04-10T19:04:15.131506Z","iopub.status.idle":"2025-04-10T19:04:15.136628Z","shell.execute_reply.started":"2025-04-10T19:04:15.131448Z","shell.execute_reply":"2025-04-10T19:04:15.135445Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Train/Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df.copy()\ny = X.pop('SalePrice')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:54:40.036789Z","iopub.execute_input":"2025-04-10T19:54:40.037247Z","iopub.status.idle":"2025-04-10T19:54:40.049674Z","shell.execute_reply.started":"2025-04-10T19:54:40.037213Z","shell.execute_reply":"2025-04-10T19:54:40.048383Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"## Drop id\nX_train = X_train.drop(columns='Id')\nX_test = X_test.drop(columns='Id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:54:41.694237Z","iopub.execute_input":"2025-04-10T19:54:41.694620Z","iopub.status.idle":"2025-04-10T19:54:41.706629Z","shell.execute_reply.started":"2025-04-10T19:54:41.694591Z","shell.execute_reply":"2025-04-10T19:54:41.705318Z"}},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":"## Process with pipeline","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeRegressor\nfrom category_encoders.woe import WOEEncoder\n\n\nX_train_processed, X_test_processed, pipeline = process_with_pipeline(X_train, X_test, y_train, threshold=0.8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:54:45.602361Z","iopub.execute_input":"2025-04-10T19:54:45.602739Z","iopub.status.idle":"2025-04-10T19:54:47.977327Z","shell.execute_reply.started":"2025-04-10T19:54:45.602710Z","shell.execute_reply":"2025-04-10T19:54:47.975869Z"}},"outputs":[{"name":"stdout","text":"Fitting DataCleaner...\nColumns with null ratio > 0.9: ['Alley', 'PoolQC', 'MiscFeature']\nColumns with same value ratio > 0.95: ['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'LowQualFinSF', 'KitchenAbvGr', 'GarageQual', 'GarageCond', '3SsnPorch', 'PoolArea', 'MiscVal']\nTransforming dataset with DataCleaner...\nDropping columns: ['KitchenAbvGr', 'MiscVal', 'PoolArea', 'GarageCond', 'Alley', 'LowQualFinSF', 'PoolQC', 'Street', 'Utilities', 'GarageQual', '3SsnPorch', 'Heating', 'Condition2', 'RoofMatl', 'MiscFeature']\nShape after cleaning: (1168, 64)\nFitting NullHandler...\nColumns with null ratio > 0.9: []\nNumeric columns to fill: ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch', 'MoSold', 'YrSold']\nCategorical columns to fill: ['MSZoning', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'PavedDrive', 'Fence', 'SaleType', 'SaleCondition']\nNumeric column 'MSSubClass' will be filled with median: 50.0\nNumeric column 'LotFrontage' will be filled with median: 70.0\nNumeric column 'LotArea' will be filled with median: 9600.0\nNumeric column 'OverallQual' will be filled with median: 6.0\nNumeric column 'OverallCond' will be filled with median: 5.0\nNumeric column 'YearBuilt' will be filled with median: 1972.0\nNumeric column 'YearRemodAdd' will be filled with median: 1994.0\nNumeric column 'MasVnrArea' will be filled with median: 0.0\nNumeric column 'BsmtFinSF1' will be filled with median: 384.5\nNumeric column 'BsmtFinSF2' will be filled with median: 0.0\nNumeric column 'BsmtUnfSF' will be filled with median: 480.0\nNumeric column 'TotalBsmtSF' will be filled with median: 997.5\nNumeric column '1stFlrSF' will be filled with median: 1095.0\nNumeric column '2ndFlrSF' will be filled with median: 0.0\nNumeric column 'GrLivArea' will be filled with median: 1473.0\nNumeric column 'BsmtFullBath' will be filled with median: 0.0\nNumeric column 'BsmtHalfBath' will be filled with median: 0.0\nNumeric column 'FullBath' will be filled with median: 2.0\nNumeric column 'HalfBath' will be filled with median: 0.0\nNumeric column 'BedroomAbvGr' will be filled with median: 3.0\nNumeric column 'TotRmsAbvGrd' will be filled with median: 6.0\nNumeric column 'Fireplaces' will be filled with median: 1.0\nNumeric column 'GarageYrBlt' will be filled with median: 1980.0\nNumeric column 'GarageCars' will be filled with median: 2.0\nNumeric column 'GarageArea' will be filled with median: 482.0\nNumeric column 'WoodDeckSF' will be filled with median: 0.0\nNumeric column 'OpenPorchSF' will be filled with median: 27.0\nNumeric column 'EnclosedPorch' will be filled with median: 0.0\nNumeric column 'ScreenPorch' will be filled with median: 0.0\nNumeric column 'MoSold' will be filled with median: 6.0\nNumeric column 'YrSold' will be filled with median: 2008.0\nCategorical column 'MSZoning' will be filled with mode: RL\nCategorical column 'LotShape' will be filled with mode: Reg\nCategorical column 'LandContour' will be filled with mode: Lvl\nCategorical column 'LotConfig' will be filled with mode: Inside\nCategorical column 'LandSlope' will be filled with mode: Gtl\nCategorical column 'Neighborhood' will be filled with mode: NAmes\nCategorical column 'Condition1' will be filled with mode: Norm\nCategorical column 'BldgType' will be filled with mode: 1Fam\nCategorical column 'HouseStyle' will be filled with mode: 1Story\nCategorical column 'RoofStyle' will be filled with mode: Gable\nCategorical column 'Exterior1st' will be filled with mode: VinylSd\nCategorical column 'Exterior2nd' will be filled with mode: VinylSd\nCategorical column 'MasVnrType' will be filled with mode: BrkFace\nCategorical column 'ExterQual' will be filled with mode: TA\nCategorical column 'ExterCond' will be filled with mode: TA\nCategorical column 'Foundation' will be filled with mode: PConc\nCategorical column 'BsmtQual' will be filled with mode: TA\nCategorical column 'BsmtCond' will be filled with mode: TA\nCategorical column 'BsmtExposure' will be filled with mode: No\nCategorical column 'BsmtFinType1' will be filled with mode: Unf\nCategorical column 'BsmtFinType2' will be filled with mode: Unf\nCategorical column 'HeatingQC' will be filled with mode: Ex\nCategorical column 'CentralAir' will be filled with mode: Y\nCategorical column 'Electrical' will be filled with mode: SBrkr\nCategorical column 'KitchenQual' will be filled with mode: TA\nCategorical column 'Functional' will be filled with mode: Typ\nCategorical column 'FireplaceQu' will be filled with mode: Gd\nCategorical column 'GarageType' will be filled with mode: Attchd\nCategorical column 'GarageFinish' will be filled with mode: Unf\nCategorical column 'PavedDrive' will be filled with mode: Y\nCategorical column 'Fence' will be filled with mode: MnPrv\nCategorical column 'SaleType' will be filled with mode: WD\nCategorical column 'SaleCondition' will be filled with mode: Normal\nTransforming dataset with NullHandler...\nFilling column 'MSSubClass' with: 50.0\nFilling column 'LotFrontage' with: 70.0\nFilling column 'LotArea' with: 9600.0\nFilling column 'OverallQual' with: 6.0\nFilling column 'OverallCond' with: 5.0\nFilling column 'YearBuilt' with: 1972.0\nFilling column 'YearRemodAdd' with: 1994.0\nFilling column 'MasVnrArea' with: 0.0\nFilling column 'BsmtFinSF1' with: 384.5\nFilling column 'BsmtFinSF2' with: 0.0\nFilling column 'BsmtUnfSF' with: 480.0\nFilling column 'TotalBsmtSF' with: 997.5\nFilling column '1stFlrSF' with: 1095.0\nFilling column '2ndFlrSF' with: 0.0\nFilling column 'GrLivArea' with: 1473.0\nFilling column 'BsmtFullBath' with: 0.0\nFilling column 'BsmtHalfBath' with: 0.0\nFilling column 'FullBath' with: 2.0\nFilling column 'HalfBath' with: 0.0\nFilling column 'BedroomAbvGr' with: 3.0\nFilling column 'TotRmsAbvGrd' with: 6.0\nFilling column 'Fireplaces' with: 1.0\nFilling column 'GarageYrBlt' with: 1980.0\nFilling column 'GarageCars' with: 2.0\nFilling column 'GarageArea' with: 482.0\nFilling column 'WoodDeckSF' with: 0.0\nFilling column 'OpenPorchSF' with: 27.0\nFilling column 'EnclosedPorch' with: 0.0\nFilling column 'ScreenPorch' with: 0.0\nFilling column 'MoSold' with: 6.0\nFilling column 'YrSold' with: 2008.0\nFilling column 'MSZoning' with: RL\nFilling column 'LotShape' with: Reg\nFilling column 'LandContour' with: Lvl\nFilling column 'LotConfig' with: Inside\nFilling column 'LandSlope' with: Gtl\nFilling column 'Neighborhood' with: NAmes\nFilling column 'Condition1' with: Norm\nFilling column 'BldgType' with: 1Fam\nFilling column 'HouseStyle' with: 1Story\nFilling column 'RoofStyle' with: Gable\nFilling column 'Exterior1st' with: VinylSd\nFilling column 'Exterior2nd' with: VinylSd\nFilling column 'MasVnrType' with: BrkFace\nFilling column 'ExterQual' with: TA\nFilling column 'ExterCond' with: TA\nFilling column 'Foundation' with: PConc\nFilling column 'BsmtQual' with: TA\nFilling column 'BsmtCond' with: TA\nFilling column 'BsmtExposure' with: No\nFilling column 'BsmtFinType1' with: Unf\nFilling column 'BsmtFinType2' with: Unf\nFilling column 'HeatingQC' with: Ex\nFilling column 'CentralAir' with: Y\nFilling column 'Electrical' with: SBrkr\nFilling column 'KitchenQual' with: TA\nFilling column 'Functional' with: Typ\nFilling column 'FireplaceQu' with: Gd\nFilling column 'GarageType' with: Attchd\nFilling column 'GarageFinish' with: Unf\nFilling column 'PavedDrive' with: Y\nFilling column 'Fence' with: MnPrv\nFilling column 'SaleType' with: WD\nFilling column 'SaleCondition' with: Normal\nShape after null handling: (1168, 64)\nFitting SelectiveOneHotEncoder...\nSelected columns for one-hot encoding (<=3 unique values): ['LandSlope', 'MasVnrType', 'CentralAir', 'GarageFinish', 'PavedDrive']\nFilling missing values in 'LandSlope' with mode: Gtl\nFilling missing values in 'MasVnrType' with mode: BrkFace\nFilling missing values in 'CentralAir' with mode: Y\nFilling missing values in 'GarageFinish' with mode: Unf\nFilling missing values in 'PavedDrive' with mode: Y\nOneHotEncoder fitted on filled data.\nOne-hot encoded columns will be: ['LandSlope_Gtl', 'LandSlope_Mod', 'LandSlope_Sev', 'MasVnrType_BrkCmn', 'MasVnrType_BrkFace', 'MasVnrType_Stone', 'CentralAir_N', 'CentralAir_Y', 'GarageFinish_Fin', 'GarageFinish_RFn', 'GarageFinish_Unf', 'PavedDrive_N', 'PavedDrive_P', 'PavedDrive_Y']\nTransforming data with SelectiveOneHotEncoder...\nFilling missing values in 'LandSlope' with: Gtl\nFilling missing values in 'MasVnrType' with: BrkFace\nFilling missing values in 'CentralAir' with: Y\nFilling missing values in 'GarageFinish' with: Unf\nFilling missing values in 'PavedDrive' with: Y\nOne-hot encoded values:\n       LandSlope_Gtl  LandSlope_Mod  LandSlope_Sev  MasVnrType_BrkCmn  MasVnrType_BrkFace  MasVnrType_Stone  CentralAir_N  CentralAir_Y  GarageFinish_Fin  GarageFinish_RFn  GarageFinish_Unf  PavedDrive_N  PavedDrive_P  PavedDrive_Y\n254             1.0            0.0            0.0                0.0                 1.0               0.0           0.0           1.0               0.0               1.0               0.0           0.0           0.0           1.0\n1066            1.0            0.0            0.0                0.0                 1.0               0.0           0.0           1.0               0.0               1.0               0.0           0.0           0.0           1.0\n638             1.0            0.0            0.0                0.0                 1.0               0.0           0.0           1.0               0.0               0.0               1.0           0.0           1.0           0.0\n799             1.0            0.0            0.0                0.0                 1.0               0.0           0.0           1.0               0.0               0.0               1.0           0.0           0.0           1.0\n380             1.0            0.0            0.0                0.0                 1.0               0.0           0.0           1.0               0.0               0.0               1.0           0.0           0.0           1.0\nFinal transformed dataframe with shape (1168, 73)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n[CorrelationFilter] Fitting...\nUsing target column 'SalePrice' for guided correlation filtering.\nCorrelation with target:\nOverallQual          0.785555\nGrLivArea            0.695652\nNeighborhood         0.681788\nKitchenQual          0.645595\nExterQual            0.641619\n                       ...   \nBsmtHalfBath         0.048346\nMasVnrType_BrkCmn    0.043160\nMoSold               0.041890\nYrSold               0.009099\nBsmtFinSF2           0.005731\nName: SalePrice, Length: 73, dtype: float64\nHighly correlated feature pairs (>0.8): [('Exterior1st', 'Exterior2nd'), ('TotalBsmtSF', '1stFlrSF'), ('GrLivArea', 'TotRmsAbvGrd'), ('GarageCars', 'GarageArea'), ('SaleType', 'SaleCondition'), ('LandSlope_Gtl', 'LandSlope_Mod'), ('MasVnrType_BrkFace', 'MasVnrType_Stone'), ('CentralAir_N', 'CentralAir_Y'), ('PavedDrive_N', 'PavedDrive_Y')]\nBetween 'Exterior1st' and 'Exterior2nd', dropping 'Exterior2nd' (lower target corr)\nBetween 'TotalBsmtSF' and '1stFlrSF', dropping '1stFlrSF' (lower target corr)\nBetween 'GrLivArea' and 'TotRmsAbvGrd', dropping 'TotRmsAbvGrd' (lower target corr)\nBetween 'GarageCars' and 'GarageArea', dropping 'GarageArea' (lower target corr)\nBetween 'SaleType' and 'SaleCondition', dropping 'SaleCondition' (lower target corr)\nBetween 'LandSlope_Gtl' and 'LandSlope_Mod', dropping 'LandSlope_Mod' (lower target corr)\nBetween 'MasVnrType_BrkFace' and 'MasVnrType_Stone', dropping 'MasVnrType_BrkFace' (lower target corr)\nBetween 'CentralAir_N' and 'CentralAir_Y', dropping 'CentralAir_Y' (lower target corr)\nBetween 'PavedDrive_N' and 'PavedDrive_Y', dropping 'PavedDrive_N' (lower target corr)\nFinal list of features to drop: ['CentralAir_Y', 'GarageArea', 'LandSlope_Mod', 'TotRmsAbvGrd', 'PavedDrive_N', '1stFlrSF', 'Exterior2nd', 'MasVnrType_BrkFace', 'SaleCondition']\n\n[CorrelationFilter] Transforming...\nDropping features: ['CentralAir_Y', 'GarageArea', 'LandSlope_Mod', 'TotRmsAbvGrd', 'PavedDrive_N', '1stFlrSF', 'Exterior2nd', 'MasVnrType_BrkFace', 'SaleCondition']\nTransforming dataset with DataCleaner...\nDropping columns: ['KitchenAbvGr', 'MiscVal', 'PoolArea', 'GarageCond', 'Alley', 'LowQualFinSF', 'PoolQC', 'Street', 'Utilities', 'GarageQual', '3SsnPorch', 'Heating', 'Condition2', 'RoofMatl', 'MiscFeature']\nShape after cleaning: (292, 64)\nTransforming dataset with NullHandler...\nFilling column 'MSSubClass' with: 50.0\nFilling column 'LotFrontage' with: 70.0\nFilling column 'LotArea' with: 9600.0\nFilling column 'OverallQual' with: 6.0\nFilling column 'OverallCond' with: 5.0\nFilling column 'YearBuilt' with: 1972.0\nFilling column 'YearRemodAdd' with: 1994.0\nFilling column 'MasVnrArea' with: 0.0\nFilling column 'BsmtFinSF1' with: 384.5\nFilling column 'BsmtFinSF2' with: 0.0\nFilling column 'BsmtUnfSF' with: 480.0\nFilling column 'TotalBsmtSF' with: 997.5\nFilling column '1stFlrSF' with: 1095.0\nFilling column '2ndFlrSF' with: 0.0\nFilling column 'GrLivArea' with: 1473.0\nFilling column 'BsmtFullBath' with: 0.0\nFilling column 'BsmtHalfBath' with: 0.0\nFilling column 'FullBath' with: 2.0\nFilling column 'HalfBath' with: 0.0\nFilling column 'BedroomAbvGr' with: 3.0\nFilling column 'TotRmsAbvGrd' with: 6.0\nFilling column 'Fireplaces' with: 1.0\nFilling column 'GarageYrBlt' with: 1980.0\nFilling column 'GarageCars' with: 2.0\nFilling column 'GarageArea' with: 482.0\nFilling column 'WoodDeckSF' with: 0.0\nFilling column 'OpenPorchSF' with: 27.0\nFilling column 'EnclosedPorch' with: 0.0\nFilling column 'ScreenPorch' with: 0.0\nFilling column 'MoSold' with: 6.0\nFilling column 'YrSold' with: 2008.0\nFilling column 'MSZoning' with: RL\nFilling column 'LotShape' with: Reg\nFilling column 'LandContour' with: Lvl\nFilling column 'LotConfig' with: Inside\nFilling column 'LandSlope' with: Gtl\nFilling column 'Neighborhood' with: NAmes\nFilling column 'Condition1' with: Norm\nFilling column 'BldgType' with: 1Fam\nFilling column 'HouseStyle' with: 1Story\nFilling column 'RoofStyle' with: Gable\nFilling column 'Exterior1st' with: VinylSd\nFilling column 'Exterior2nd' with: VinylSd\nFilling column 'MasVnrType' with: BrkFace\nFilling column 'ExterQual' with: TA\nFilling column 'ExterCond' with: TA\nFilling column 'Foundation' with: PConc\nFilling column 'BsmtQual' with: TA\nFilling column 'BsmtCond' with: TA\nFilling column 'BsmtExposure' with: No\nFilling column 'BsmtFinType1' with: Unf\nFilling column 'BsmtFinType2' with: Unf\nFilling column 'HeatingQC' with: Ex\nFilling column 'CentralAir' with: Y\nFilling column 'Electrical' with: SBrkr\nFilling column 'KitchenQual' with: TA\nFilling column 'Functional' with: Typ\nFilling column 'FireplaceQu' with: Gd\nFilling column 'GarageType' with: Attchd\nFilling column 'GarageFinish' with: Unf\nFilling column 'PavedDrive' with: Y\nFilling column 'Fence' with: MnPrv\nFilling column 'SaleType' with: WD\nFilling column 'SaleCondition' with: Normal\nShape after null handling: (292, 64)\nTransforming data with SelectiveOneHotEncoder...\nFilling missing values in 'LandSlope' with: Gtl\nFilling missing values in 'MasVnrType' with: BrkFace\nFilling missing values in 'CentralAir' with: Y\nFilling missing values in 'GarageFinish' with: Unf\nFilling missing values in 'PavedDrive' with: Y\nOne-hot encoded values:\n       LandSlope_Gtl  LandSlope_Mod  LandSlope_Sev  MasVnrType_BrkCmn  MasVnrType_BrkFace  MasVnrType_Stone  CentralAir_N  CentralAir_Y  GarageFinish_Fin  GarageFinish_RFn  GarageFinish_Unf  PavedDrive_N  PavedDrive_P  PavedDrive_Y\n892             1.0            0.0            0.0                0.0                 1.0               0.0           0.0           1.0               0.0               1.0               0.0           0.0           0.0           1.0\n1105            1.0            0.0            0.0                0.0                 1.0               0.0           0.0           1.0               0.0               1.0               0.0           0.0           0.0           1.0\n413             1.0            0.0            0.0                0.0                 1.0               0.0           0.0           1.0               0.0               0.0               1.0           0.0           0.0           1.0\n522             1.0            0.0            0.0                0.0                 1.0               0.0           0.0           1.0               0.0               0.0               1.0           0.0           0.0           1.0\n1036            1.0            0.0            0.0                0.0                 0.0               1.0           0.0           1.0               1.0               0.0               0.0           0.0           0.0           1.0\nFinal transformed dataframe with shape (292, 73)\n\n[CorrelationFilter] Transforming...\nDropping features: ['CentralAir_Y', 'GarageArea', 'LandSlope_Mod', 'TotRmsAbvGrd', 'PavedDrive_N', '1stFlrSF', 'Exterior2nd', 'MasVnrType_BrkFace', 'SaleCondition']\n🏃 View run house_price_preprocessing at: https://dagshub.com/ekvirika/HousePrices.mlflow/#/experiments/5/runs/d5dff3d566e34029948573242b346799\n🧪 View experiment at: https://dagshub.com/ekvirika/HousePrices.mlflow/#/experiments/5\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"X_train_processed.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For tree models\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeRegressor\nfrom category_encoders.woe import WOEEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\n\nX_train_processed, X_test_processed, tree_pipeline = process_with_tree_pipeline(X_train, X_test, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:08:38.157219Z","iopub.execute_input":"2025-04-10T19:08:38.157635Z","iopub.status.idle":"2025-04-10T19:08:39.416154Z","shell.execute_reply.started":"2025-04-10T19:08:38.157597Z","shell.execute_reply":"2025-04-10T19:08:39.415019Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Fitting DataCleaner...\nColumns with null ratio > 0.99: ['PoolQC']\nColumns with same value ratio > 0.95: ['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'LowQualFinSF', 'KitchenAbvGr', 'GarageQual', 'GarageCond', '3SsnPorch', 'PoolArea', 'MiscVal']\nTransforming dataset with DataCleaner...\nDropping columns: ['KitchenAbvGr', 'MiscVal', 'PoolArea', 'GarageCond', 'LowQualFinSF', 'PoolQC', 'Street', 'Utilities', 'GarageQual', '3SsnPorch', 'Heating', 'Condition2', 'RoofMatl']\nShape after cleaning: (1168, 66)\nFitting NullHandler...\nColumns with null ratio > 0.99: []\nNumeric columns to fill: ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch', 'MoSold', 'YrSold']\nCategorical columns to fill: ['MSZoning', 'Alley', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'PavedDrive', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\nNumeric column 'MSSubClass' will be filled with median: 50.0\nNumeric column 'LotFrontage' will be filled with median: 70.0\nNumeric column 'LotArea' will be filled with median: 9600.0\nNumeric column 'OverallQual' will be filled with median: 6.0\nNumeric column 'OverallCond' will be filled with median: 5.0\nNumeric column 'YearBuilt' will be filled with median: 1972.0\nNumeric column 'YearRemodAdd' will be filled with median: 1994.0\nNumeric column 'MasVnrArea' will be filled with median: 0.0\nNumeric column 'BsmtFinSF1' will be filled with median: 384.5\nNumeric column 'BsmtFinSF2' will be filled with median: 0.0\nNumeric column 'BsmtUnfSF' will be filled with median: 480.0\nNumeric column 'TotalBsmtSF' will be filled with median: 997.5\nNumeric column '1stFlrSF' will be filled with median: 1095.0\nNumeric column '2ndFlrSF' will be filled with median: 0.0\nNumeric column 'GrLivArea' will be filled with median: 1473.0\nNumeric column 'BsmtFullBath' will be filled with median: 0.0\nNumeric column 'BsmtHalfBath' will be filled with median: 0.0\nNumeric column 'FullBath' will be filled with median: 2.0\nNumeric column 'HalfBath' will be filled with median: 0.0\nNumeric column 'BedroomAbvGr' will be filled with median: 3.0\nNumeric column 'TotRmsAbvGrd' will be filled with median: 6.0\nNumeric column 'Fireplaces' will be filled with median: 1.0\nNumeric column 'GarageYrBlt' will be filled with median: 1980.0\nNumeric column 'GarageCars' will be filled with median: 2.0\nNumeric column 'GarageArea' will be filled with median: 482.0\nNumeric column 'WoodDeckSF' will be filled with median: 0.0\nNumeric column 'OpenPorchSF' will be filled with median: 27.0\nNumeric column 'EnclosedPorch' will be filled with median: 0.0\nNumeric column 'ScreenPorch' will be filled with median: 0.0\nNumeric column 'MoSold' will be filled with median: 6.0\nNumeric column 'YrSold' will be filled with median: 2008.0\nCategorical column 'MSZoning' will be filled with mode: RL\nCategorical column 'Alley' will be filled with mode: Grvl\nCategorical column 'LotShape' will be filled with mode: Reg\nCategorical column 'LandContour' will be filled with mode: Lvl\nCategorical column 'LotConfig' will be filled with mode: Inside\nCategorical column 'LandSlope' will be filled with mode: Gtl\nCategorical column 'Neighborhood' will be filled with mode: NAmes\nCategorical column 'Condition1' will be filled with mode: Norm\nCategorical column 'BldgType' will be filled with mode: 1Fam\nCategorical column 'HouseStyle' will be filled with mode: 1Story\nCategorical column 'RoofStyle' will be filled with mode: Gable\nCategorical column 'Exterior1st' will be filled with mode: VinylSd\nCategorical column 'Exterior2nd' will be filled with mode: VinylSd\nCategorical column 'MasVnrType' will be filled with mode: BrkFace\nCategorical column 'ExterQual' will be filled with mode: TA\nCategorical column 'ExterCond' will be filled with mode: TA\nCategorical column 'Foundation' will be filled with mode: PConc\nCategorical column 'BsmtQual' will be filled with mode: TA\nCategorical column 'BsmtCond' will be filled with mode: TA\nCategorical column 'BsmtExposure' will be filled with mode: No\nCategorical column 'BsmtFinType1' will be filled with mode: Unf\nCategorical column 'BsmtFinType2' will be filled with mode: Unf\nCategorical column 'HeatingQC' will be filled with mode: Ex\nCategorical column 'CentralAir' will be filled with mode: Y\nCategorical column 'Electrical' will be filled with mode: SBrkr\nCategorical column 'KitchenQual' will be filled with mode: TA\nCategorical column 'Functional' will be filled with mode: Typ\nCategorical column 'FireplaceQu' will be filled with mode: Gd\nCategorical column 'GarageType' will be filled with mode: Attchd\nCategorical column 'GarageFinish' will be filled with mode: Unf\nCategorical column 'PavedDrive' will be filled with mode: Y\nCategorical column 'Fence' will be filled with mode: MnPrv\nCategorical column 'MiscFeature' will be filled with mode: Shed\nCategorical column 'SaleType' will be filled with mode: WD\nCategorical column 'SaleCondition' will be filled with mode: Normal\nTransforming dataset with NullHandler...\nFilling column 'MSSubClass' with: 50.0\nFilling column 'LotFrontage' with: 70.0\nFilling column 'LotArea' with: 9600.0\nFilling column 'OverallQual' with: 6.0\nFilling column 'OverallCond' with: 5.0\nFilling column 'YearBuilt' with: 1972.0\nFilling column 'YearRemodAdd' with: 1994.0\nFilling column 'MasVnrArea' with: 0.0\nFilling column 'BsmtFinSF1' with: 384.5\nFilling column 'BsmtFinSF2' with: 0.0\nFilling column 'BsmtUnfSF' with: 480.0\nFilling column 'TotalBsmtSF' with: 997.5\nFilling column '1stFlrSF' with: 1095.0\nFilling column '2ndFlrSF' with: 0.0\nFilling column 'GrLivArea' with: 1473.0\nFilling column 'BsmtFullBath' with: 0.0\nFilling column 'BsmtHalfBath' with: 0.0\nFilling column 'FullBath' with: 2.0\nFilling column 'HalfBath' with: 0.0\nFilling column 'BedroomAbvGr' with: 3.0\nFilling column 'TotRmsAbvGrd' with: 6.0\nFilling column 'Fireplaces' with: 1.0\nFilling column 'GarageYrBlt' with: 1980.0\nFilling column 'GarageCars' with: 2.0\nFilling column 'GarageArea' with: 482.0\nFilling column 'WoodDeckSF' with: 0.0\nFilling column 'OpenPorchSF' with: 27.0\nFilling column 'EnclosedPorch' with: 0.0\nFilling column 'ScreenPorch' with: 0.0\nFilling column 'MoSold' with: 6.0\nFilling column 'YrSold' with: 2008.0\nFilling column 'MSZoning' with: RL\nFilling column 'Alley' with: Grvl\nFilling column 'LotShape' with: Reg\nFilling column 'LandContour' with: Lvl\nFilling column 'LotConfig' with: Inside\nFilling column 'LandSlope' with: Gtl\nFilling column 'Neighborhood' with: NAmes\nFilling column 'Condition1' with: Norm\nFilling column 'BldgType' with: 1Fam\nFilling column 'HouseStyle' with: 1Story\nFilling column 'RoofStyle' with: Gable\nFilling column 'Exterior1st' with: VinylSd\nFilling column 'Exterior2nd' with: VinylSd\nFilling column 'MasVnrType' with: BrkFace\nFilling column 'ExterQual' with: TA\nFilling column 'ExterCond' with: TA\nFilling column 'Foundation' with: PConc\nFilling column 'BsmtQual' with: TA\nFilling column 'BsmtCond' with: TA\nFilling column 'BsmtExposure' with: No\nFilling column 'BsmtFinType1' with: Unf\nFilling column 'BsmtFinType2' with: Unf\nFilling column 'HeatingQC' with: Ex\nFilling column 'CentralAir' with: Y\nFilling column 'Electrical' with: SBrkr\nFilling column 'KitchenQual' with: TA\nFilling column 'Functional' with: Typ\nFilling column 'FireplaceQu' with: Gd\nFilling column 'GarageType' with: Attchd\nFilling column 'GarageFinish' with: Unf\nFilling column 'PavedDrive' with: Y\nFilling column 'Fence' with: MnPrv\nFilling column 'MiscFeature' with: Shed\nFilling column 'SaleType' with: WD\nFilling column 'SaleCondition' with: Normal\nShape after null handling: (1168, 66)\nTransforming dataset with DataCleaner...\nDropping columns: ['KitchenAbvGr', 'MiscVal', 'PoolArea', 'GarageCond', 'LowQualFinSF', 'PoolQC', 'Street', 'Utilities', 'GarageQual', '3SsnPorch', 'Heating', 'Condition2', 'RoofMatl']\nShape after cleaning: (292, 66)\nTransforming dataset with NullHandler...\nFilling column 'MSSubClass' with: 50.0\nFilling column 'LotFrontage' with: 70.0\nFilling column 'LotArea' with: 9600.0\nFilling column 'OverallQual' with: 6.0\nFilling column 'OverallCond' with: 5.0\nFilling column 'YearBuilt' with: 1972.0\nFilling column 'YearRemodAdd' with: 1994.0\nFilling column 'MasVnrArea' with: 0.0\nFilling column 'BsmtFinSF1' with: 384.5\nFilling column 'BsmtFinSF2' with: 0.0\nFilling column 'BsmtUnfSF' with: 480.0\nFilling column 'TotalBsmtSF' with: 997.5\nFilling column '1stFlrSF' with: 1095.0\nFilling column '2ndFlrSF' with: 0.0\nFilling column 'GrLivArea' with: 1473.0\nFilling column 'BsmtFullBath' with: 0.0\nFilling column 'BsmtHalfBath' with: 0.0\nFilling column 'FullBath' with: 2.0\nFilling column 'HalfBath' with: 0.0\nFilling column 'BedroomAbvGr' with: 3.0\nFilling column 'TotRmsAbvGrd' with: 6.0\nFilling column 'Fireplaces' with: 1.0\nFilling column 'GarageYrBlt' with: 1980.0\nFilling column 'GarageCars' with: 2.0\nFilling column 'GarageArea' with: 482.0\nFilling column 'WoodDeckSF' with: 0.0\nFilling column 'OpenPorchSF' with: 27.0\nFilling column 'EnclosedPorch' with: 0.0\nFilling column 'ScreenPorch' with: 0.0\nFilling column 'MoSold' with: 6.0\nFilling column 'YrSold' with: 2008.0\nFilling column 'MSZoning' with: RL\nFilling column 'Alley' with: Grvl\nFilling column 'LotShape' with: Reg\nFilling column 'LandContour' with: Lvl\nFilling column 'LotConfig' with: Inside\nFilling column 'LandSlope' with: Gtl\nFilling column 'Neighborhood' with: NAmes\nFilling column 'Condition1' with: Norm\nFilling column 'BldgType' with: 1Fam\nFilling column 'HouseStyle' with: 1Story\nFilling column 'RoofStyle' with: Gable\nFilling column 'Exterior1st' with: VinylSd\nFilling column 'Exterior2nd' with: VinylSd\nFilling column 'MasVnrType' with: BrkFace\nFilling column 'ExterQual' with: TA\nFilling column 'ExterCond' with: TA\nFilling column 'Foundation' with: PConc\nFilling column 'BsmtQual' with: TA\nFilling column 'BsmtCond' with: TA\nFilling column 'BsmtExposure' with: No\nFilling column 'BsmtFinType1' with: Unf\nFilling column 'BsmtFinType2' with: Unf\nFilling column 'HeatingQC' with: Ex\nFilling column 'CentralAir' with: Y\nFilling column 'Electrical' with: SBrkr\nFilling column 'KitchenQual' with: TA\nFilling column 'Functional' with: Typ\nFilling column 'FireplaceQu' with: Gd\nFilling column 'GarageType' with: Attchd\nFilling column 'GarageFinish' with: Unf\nFilling column 'PavedDrive' with: Y\nFilling column 'Fence' with: MnPrv\nFilling column 'MiscFeature' with: Shed\nFilling column 'SaleType' with: WD\nFilling column 'SaleCondition' with: Normal\nShape after null handling: (292, 66)\n🏃 View run tree_model_preprocessing at: https://dagshub.com/ekvirika/HousePrices.mlflow/#/experiments/5/runs/15692e7f73224d3f86094ea91d617305\n🧪 View experiment at: https://dagshub.com/ekvirika/HousePrices.mlflow/#/experiments/5\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# linear models","metadata":{}},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport mlflow\nimport mlflow.sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_log_error\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set MLflow experiment\nexperiment_name = \"House Price Regression\"\nmlflow.set_experiment(experiment_name)\n\n# Initialize model\nmodel = LinearRegression()\n\n# Start MLflow run\nwith mlflow.start_run() as run:\n    # Set a custom name for the run\n    mlflow.set_tag(\"mlflow.runName\", \"Linear_Regression_v2\")\n\n    # Log model parameters (none for basic LinearRegression, but for demo let's log default ones)\n    mlflow.log_param(\"fit_intercept\", model.fit_intercept)\n    mlflow.log_param(\"model_type\", \"LinearRegression\")\n\n    # Train the model\n    model.fit(X_train_processed, y_train)\n    y_pred = model.predict(X_test_processed)\n\n    y_pred_clipped = np.maximum(y_pred, 1)\n    y_test_clipped = np.maximum(y_test, 1)\n\n    # Calculate metrics\n    mae = mean_absolute_error(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_test, y_pred)\n    rmsle = np.sqrt(mean_squared_log_error(y_test_clipped, y_pred_clipped))\n\n    # Log metrics\n    mlflow.log_metric(\"MAE\", mae)\n    mlflow.log_metric(\"MSE\", mse)\n    mlflow.log_metric(\"RMSE\", rmse)\n    mlflow.log_metric(\"R2\", r2)\n    mlflow.log_metric(\"RMSLE\", rmsle)\n\n    # Log the model\n    mlflow.sklearn.log_model(model, artifact_path=\"model\", registered_model_name=\"HousePricesLinearModel\")\n\n    # Plot and save prediction vs actual plot\n    plt.figure(figsize=(6, 6))\n    sns.scatterplot(x=y_test, y=y_pred)\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n    plt.xlabel(\"Actual Prices\")\n    plt.ylabel(\"Predicted Prices\")\n    plt.title(\"Predicted vs Actual Prices\")\n    plt.tight_layout()\n    plt.savefig(\"scatter_plot.png\")\n\n    # Log the plot image\n    mlflow.log_artifact(\"scatter_plot.png\")\n    mlflow.end_run()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport mlflow\nimport mlflow.sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_log_error\n\n# Set MLflow experiment\nexperiment_name = \"House Price Regression\"\nmlflow.set_experiment(experiment_name)\n\n# Number of folds\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nfold = 1\nfor train_index, test_index in kf.split(X_train_processed):\n    with mlflow.start_run(run_name=f\"LinearRegression_Fold_{fold}\"):\n        # Split the data\n        X_train, X_test = X_train_processed.iloc[train_index], X_train_processed.iloc[test_index]  # .iloc for DataFrame\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]  # .iloc for Series/DataFrame\n\n        # Initialize model\n        model = LinearRegression()\n\n        # Log parameters\n        mlflow.log_param(\"fit_intercept\", model.fit_intercept)\n        mlflow.log_param(\"model_type\", \"LinearRegression\")\n        mlflow.log_param(\"fold\", fold)\n\n        # Train the model\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n\n        # Clip negative predictions for RMSLE\n        y_pred_clipped = np.maximum(y_pred, 1)\n        y_test_clipped = np.maximum(y_test, 1)\n\n        # Metrics\n        mae = mean_absolute_error(y_test, y_pred)\n        mse = mean_squared_error(y_test, y_pred)\n        rmse = np.sqrt(mse)\n        r2 = r2_score(y_test, y_pred)\n        rmsle = rmsle(y_test_clipped, y_pred_clipped)\n\n        # Log metrics\n        mlflow.log_metric(\"MAE\", mae)\n        mlflow.log_metric(\"MSE\", mse)\n        mlflow.log_metric(\"RMSE\", rmse)\n        mlflow.log_metric(\"R2\", r2)\n        mlflow.log_metric(\"RMSLE\", rmsle)\n\n        # Log model\n        mlflow.sklearn.log_model(model, artifact_path=f\"model_fold_{fold}\")\n\n        # Plot predictions\n        plt.figure(figsize=(6, 6))\n        sns.scatterplot(x=y_test, y=y_pred)\n        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n        plt.xlabel(\"Actual Prices\")\n        plt.ylabel(\"Predicted Prices\")\n        plt.title(f\"Predicted vs Actual Prices (Fold {fold})\")\n        plt.tight_layout()\n\n        plot_name = f\"scatter_plot_fold_{fold}.png\"\n        plt.savefig(plot_name)\n        mlflow.log_artifact(plot_name)\n\n        fold += 1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Linear Regression with GridSearch and RFE","metadata":{}},{"cell_type":"code","source":"X_train_processed.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport mlflow\nimport mlflow.sklearn\nimport numpy as np\nimport pandas as pd\n\nexperiment_name = \"House Price Regression\"\nrun_name = \"Linear_Regression_RFE_v1\"\nmlflow.set_experiment(experiment_name)\n\nbase_model = LinearRegression()\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('feature_selector', RFE(estimator=base_model)),\n    ('regressor', base_model)\n])\n\nparam_grid = {\n    'feature_selector__n_features_to_select': [20, 25, 40, 60],\n}\n\n\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_root_mean_squared_error')\n\n# MLflow tracking\nwith mlflow.start_run(run_name=run_name) as run:\n    # Fit the model\n    grid_search.fit(X_train_processed, y_train)\n    best_model = grid_search.best_estimator_\n    best_params = grid_search.best_params_\n    \n    # Get predictions\n    y_pred = best_model.predict(X_test_processed)\n    \n    # Metrics\n    mae = mean_absolute_error(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_test, y_pred)\n    y_test_clipped = np.maximum(0, y_test)\n    y_pred_clipped = np.maximum(0, y_pred)\n    log_rmse = np.sqrt(mean_squared_error(np.log1p(y_test_clipped), np.log1p(y_pred_clipped)))\n\n    \n    # Log model type\n    mlflow.log_param(\"model_type\", \"LinearRegression\")\n    \n    # Log best parameters from GridSearch\n    for param_name, param_value in best_params.items():\n        mlflow.log_param(param_name, param_value)\n    \n    # NEW: Log the selected features\n    feature_selector = best_model.named_steps['feature_selector']\n    selected_features_mask = feature_selector.support_\n    feature_names = X_train_processed.columns.tolist()\n    selected_features = [feature for feature, selected in zip(feature_names, selected_features_mask) if selected]\n    \n    # Log selected features as a parameter (as string)\n    mlflow.log_param(\"selected_features\", str(selected_features))\n    \n    # Save and log the selected features as a CSV\n    selected_features_df = pd.DataFrame({\n        'feature_name': feature_names,\n        'selected': selected_features_mask\n    })\n    selected_features_path = \"selected_features.csv\"\n    selected_features_df.to_csv(selected_features_path, index=False)\n    mlflow.log_artifact(selected_features_path)\n    \n    # Log metrics\n    mlflow.log_metric(\"MAE\", mae)\n    mlflow.log_metric(\"MSE\", mse)\n    mlflow.log_metric(\"RMSE\", rmse)\n    mlflow.log_metric(\"R2\", r2)\n    mlflow.log_metric(\"Log_RMSE\", log_rmse)\n\n    \n    # NEW: Log optimized RMSE (with validation scores)\n    cv_rmse_scores = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n    best_cv_rmse = cv_rmse_scores[grid_search.best_index_]\n    mlflow.log_metric(\"best_cv_rmse\", best_cv_rmse)\n    mlflow.log_metric(\"rmse_std\", np.std(cv_rmse_scores))\n    \n    # Log the final model\n    mlflow.sklearn.log_model(best_model, \"model\", registered_model_name=\"HousePricesLinearModel_RFE\")\n\n    from sklearn.inspection import permutation_importance\n    \n    result = permutation_importance(best_model, X_test_processed, y_test, n_repeats=10, random_state=42)\n    \n    perm_sorted_idx = result.importances_mean.argsort()[::-1]\n    features = np.array(X_train_processed.columns)[perm_sorted_idx]\n    importances = result.importances_mean[perm_sorted_idx]\n    \n    # Plot and log\n    plt.figure(figsize=(10, 6))\n    plt.barh(features, importances)\n    plt.xlabel(\"Permutation Importance\")\n    plt.title(\"Permutation Feature Importance\")\n    plt.tight_layout()\n    plt.savefig(\"permutation_importance.png\")\n    mlflow.log_artifact(\"permutation_importance.png\")\n\n\n    import shap\n\n    explainer = shap.Explainer(best_model.named_steps['regressor'], X_train_processed[selected_features])\n    shap_values = explainer(X_test_processed[selected_features])\n    \n    # Summary plot\n    shap.summary_plot(shap_values, X_test_processed[selected_features], show=False)\n    plt.tight_layout()\n    plt.savefig(\"shap_summary.png\")\n    mlflow.log_artifact(\"shap_summary.png\")\n\n    \n    \n    # Log feature importances if available\n    try:\n        if hasattr(best_model.named_steps['regressor'], 'coef_'):\n            # For linear models, we can get coefficients\n            coef = best_model.named_steps['regressor'].coef_\n            # Only get coefficients for selected features\n            selected_feature_coefs = [(feature, coef) for feature, selected, coef in \n                                     zip(feature_names, selected_features_mask, coef) if selected]\n            \n            # Create and log feature importance plot\n            import matplotlib.pyplot as plt\n            plt.figure(figsize=(10, 6))\n            features, importance = zip(*selected_feature_coefs)\n            plt.barh(features, importance)\n            plt.xlabel('Coefficient Value')\n            plt.title('Feature Importance')\n            plt.tight_layout()\n            plt.savefig('feature_importance.png')\n            mlflow.log_artifact('feature_importance.png')\n    except Exception as e:\n        # Log any errors that occur when extracting feature importance\n        mlflow.log_param(\"feature_importance_error\", str(e))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ridge Regression","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport mlflow\nimport mlflow.sklearn\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\n# Set up MLflow experiment\nexperiment_name = \"House Price Regression\"\nrun_name = \"Ridge_RFE_v3\"\nexperiment_description = \"Grid search for Ridge regression with RFE using different alpha values, optimizing for RMSE log\"\n\n# Check if experiment exists, otherwise create it\ntry:\n    experiment_id = mlflow.create_experiment(\n        experiment_name, \n        tags={\"description\": experiment_description}\n    )\nexcept mlflow.exceptions.MlflowException:\n    experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n\n# Set the active experiment\nmlflow.set_experiment(experiment_name)\n\n# Define a custom scorer that calculates RMSE on log-transformed values\ndef rmse_log_scorer(estimator, X, y):\n    y_pred = estimator.predict(X)\n    # Handle potential negative predictions\n    y_pred = np.maximum(y_pred, 1e-10)  # Set a small positive minimum value\n    log_y = np.log(y)\n    log_y_pred = np.log(y_pred)\n    mse_log = mean_squared_error(log_y, log_y_pred)\n    rmse_log = np.sqrt(mse_log)\n    return -rmse_log  # Negative because GridSearchCV maximizes score\n\n# Define the parameter grid\nparam_grid = {\n    'ridge__alpha': [50.0, 60.0, 80.0, 100.0],\n    'rfe__n_features_to_select': [54, 55, 56]  \n}\n\n# Create pipeline with RFE and Ridge\nridge = Ridge()\nrfe = RFE(estimator=Ridge(), step=1)\npipeline = Pipeline([\n    ('rfe', rfe),\n    ('ridge', ridge)\n])\n\n# Create the grid search\ngrid_search = GridSearchCV(\n    pipeline,\n    param_grid=param_grid,\n    scoring=rmse_log_scorer,\n    cv=5,\n    verbose=1,\n    n_jobs=-1  # Use all available cores\n)\n\n# Assuming X_train_processed and y_train are your training data\n# Fit the grid search\nprint(\"Starting grid search...\")\ngrid_search.fit(X_train_processed, y_train)\nprint(\"Grid search completed!\")\n\n# Get the best parameters and best score\nbest_params = grid_search.best_params_\nbest_score = -grid_search.best_score_  # Convert back to positive RMSE\nprint(f\"Best parameters: {best_params}\")\nprint(f\"Best RMSE log score: {best_score:.4f}\")\n\n# Get the best model\nbest_model = grid_search.best_estimator_\n\n# Evaluate the best model on test data\ny_pred = best_model.predict(X_test_processed)\n\n# Calculate metrics\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\n# Calculate RMSE log\ny_pred = np.maximum(y_pred, 1e-10)  # Ensure positive values\nlog_y_test = np.log(y_test)\nlog_y_pred = np.log(y_pred)\nmse_log = mean_squared_error(log_y_test, log_y_pred)\nrmse_log = np.sqrt(mse_log)\n\nprint(f\"Test MAE:     {mae:.4f}\")\nprint(f\"Test MSE:     {mse:.4f}\")\nprint(f\"Test RMSE:    {rmse:.4f}\")\nprint(f\"Test R²:      {r2:.4f}\")\nprint(f\"Test RMSE_log: {rmse_log:.4f}\")\n\n# Log the best model and results to MLflow\nwith mlflow.start_run(run_name=run_name):\n    # Log all parameters\n    mlflow.log_param(\"alpha\", best_params['ridge__alpha'])\n    mlflow.log_param(\"n_features_selected\", best_params['rfe__n_features_to_select'])\n    mlflow.log_param(\"total_features\", X_train_processed.shape[1])\n    \n    # Log metrics\n    mlflow.log_metrics({\n        \"mae\": mae,\n        \"mse\": mse,\n        \"rmse\": rmse,\n        \"r2\": r2,\n        \"rmse_log\": rmse_log,\n        \"cv_rmse_log\": best_score\n    })\n    \n    # Log all grid search results as artifacts\n    cv_results = pd.DataFrame(grid_search.cv_results_)\n    cv_results.to_csv(\"grid_search_results.csv\", index=False)\n    mlflow.log_artifact(\"grid_search_results.csv\")\n    \n    # Log the best model\n    mlflow.sklearn.log_model(best_model, \"ridge_rfe_model\")\n    \n    # Log which features were selected\n    if hasattr(best_model.named_steps['rfe'], 'support_'):\n        selected_features = np.where(best_model.named_steps['rfe'].support_)[0]\n        feature_ranking = best_model.named_steps['rfe'].ranking_\n        \n        # If you have feature names available, use them\n        # Assuming feature_names is a list of feature names\n        try:\n            feature_names = X_train_processed.columns.tolist()\n            selected_feature_names = [feature_names[i] for i in selected_features]\n            with open(\"selected_features.txt\", \"w\") as f:\n                f.write(\"Selected features:\\n\")\n                for name in selected_feature_names:\n                    f.write(f\"- {name}\\n\")\n                \n                f.write(\"\\nFeature rankings:\\n\")\n                for i, name in enumerate(feature_names):\n                    f.write(f\"- {name}: {feature_ranking[i]}\\n\")\n            \n            mlflow.log_artifact(\"selected_features.txt\")\n        except:\n            # If feature names are not available\n            np.savetxt(\"selected_features.txt\", selected_features)\n            np.savetxt(\"feature_ranking.txt\", feature_ranking)\n            mlflow.log_artifact(\"selected_features.txt\")\n            mlflow.log_artifact(\"feature_ranking.txt\")\n    \n    # Log model type and description\n    mlflow.set_tags({\n        \"model_type\": \"Ridge Regression with RFE\",\n        \"description\": \"Grid search over alpha values and number of features using RFE\",\n        \"optimization_metric\": \"RMSE on log-transformed predictions\"\n    })\n    \n    run_id = mlflow.active_run().info.run_id\n    print(f\"Run ID: {run_id}\")\n    print(f\"Experiment ID: {experiment_id}\")\n    print(\"Model and results successfully logged to MLflow!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Lasso Regression (L1 Regularization)","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport mlflow\nimport mlflow.sklearn\nimport numpy as np\nimport pandas as pd\n\nexperiment_name = \"House Price Regression\"\nrun_name = \"Lasso_Regression_v4\"\nmlflow.set_experiment(experiment_name)\n\n# Base model - Lasso regression\nbase_model = Lasso()\n\n# Pipeline\n# Modified pipeline with adjusted parameters\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('feature_selector', RFE(estimator=Lasso(max_iter=10000, tol=0.01))),\n    ('regressor', Lasso(max_iter=10000, tol=0.01))  # Increased max_iter and relaxed tolerance\n])\n\n# Modified parameter grid with more reasonable alpha values\nparam_grid = {\n    'feature_selector__n_features_to_select': [18, 20, 22],\n    'regressor__alpha': [ 1500.0, 2000.0, 2500.0],  # Lower alpha values to try\n}\n\n# GridSearchCV\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_root_mean_squared_error')\n\n# MLflow tracking\nwith mlflow.start_run(run_name=run_name) as run:\n    # Fit the model\n    grid_search.fit(X_train_processed, y_train)\n    best_model = grid_search.best_estimator_\n    best_params = grid_search.best_params_\n    \n    # Get predictions\n    y_pred = best_model.predict(X_test_processed)\n    \n    # Metrics\n    mae = mean_absolute_error(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_test, y_pred)\n    y_test_clipped = np.maximum(0, y_test)\n    y_pred_clipped = np.maximum(0, y_pred)\n    log_rmse = np.sqrt(mean_squared_error(np.log1p(y_test_clipped), np.log1p(y_pred_clipped)))\n\n    \n    # Log model type\n    mlflow.log_param(\"model_type\", \"LassoRegression\")\n    \n    # Log best parameters from GridSearch\n    for param_name, param_value in best_params.items():\n        mlflow.log_param(param_name, param_value)\n    \n    # Log the selected features\n    feature_selector = best_model.named_steps['feature_selector']\n    selected_features_mask = feature_selector.support_\n    feature_names = X_train_processed.columns.tolist()\n    selected_features = [feature for feature, selected in zip(feature_names, selected_features_mask) if selected]\n    \n    # Log selected features as a parameter (as string)\n    mlflow.log_param(\"selected_features\", str(selected_features))\n    \n    # Save and log the selected features as a CSV\n    selected_features_df = pd.DataFrame({\n        'feature_name': feature_names,\n        'selected': selected_features_mask\n    })\n    selected_features_path = \"selected_features.csv\"\n    selected_features_df.to_csv(selected_features_path, index=False)\n    mlflow.log_artifact(selected_features_path)\n    \n    # Log metrics\n    mlflow.log_metric(\"MAE\", mae)\n    mlflow.log_metric(\"MSE\", mse)\n    mlflow.log_metric(\"RMSE\", rmse)\n    mlflow.log_metric(\"R2\", r2)\n    mlflow.log_metric(\"Log_RMSE\", log_rmse)\n\n    # Log optimized RMSE (with validation scores)\n    cv_rmse_scores = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n    best_cv_rmse = cv_rmse_scores[grid_search.best_index_]\n    mlflow.log_metric(\"best_cv_rmse\", best_cv_rmse)\n    mlflow.log_metric(\"rmse_std\", np.std(cv_rmse_scores))\n    \n    # Log the final model\n    mlflow.sklearn.log_model(best_model, \"model\", registered_model_name=\"HousePricesLassoModel_RFE\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tree Models","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport mlflow\nimport mlflow.sklearn  # Optional\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:18:34.014155Z","iopub.execute_input":"2025-04-10T19:18:34.014597Z","iopub.status.idle":"2025-04-10T19:18:34.020004Z","shell.execute_reply.started":"2025-04-10T19:18:34.014566Z","shell.execute_reply":"2025-04-10T19:18:34.018734Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def evaluate_model(model, model_name, X_train, y_train, X_test, y_test, log_mlflow=False):\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    rmse = mean_squared_error(y_test, preds, squared=False)\n    r2 = r2_score(y_test, preds)\n\n    print(f\"{model_name} RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n\n    if log_mlflow:\n        with mlflow.start_run(run_name=model_name):\n            mlflow.log_param(\"model\", model_name)\n            mlflow.log_metric(\"rmse\", rmse)\n            mlflow.log_metric(\"r2\", r2)\n            \n            mlflow.sklearn.log_model(model, model_name)\n\n    return model, rmse, r2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:18:35.937076Z","iopub.execute_input":"2025-04-10T19:18:35.937431Z","iopub.status.idle":"2025-04-10T19:18:35.943962Z","shell.execute_reply.started":"2025-04-10T19:18:35.937405Z","shell.execute_reply":"2025-04-10T19:18:35.942823Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\n\nmodels = {\n    \"RandomForest\": RandomForestRegressor(random_state=42),\n    \"GradientBoosting\": GradientBoostingRegressor(random_state=42),\n    \"Bagging\": BaggingRegressor(random_state=42),\n    \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n    \"XGBoost\": XGBRegressor(random_state=42, objective='reg:squarederror')\n}\n\nparam_grids = {\n    \"RandomForest\": {\n        \"n_estimators\": [150, 200, 250, 300],\n        \"max_depth\": [15, 20, 25, 30]\n    },\n    \"GradientBoosting\": {\n        \"n_estimators\": [150, 200, 250],\n        \"learning_rate\": [0.05, 0.1, 1, 10.0],\n        \"max_depth\": [2, 3, 4]\n    },\n    \"Bagging\": {\n        \"n_estimators\": [10, 50],\n        \"max_samples\": [0.5, 1.0]\n    },\n    \"DecisionTree\": {\n        \"max_depth\": [None, 10, 20],\n        \"min_samples_split\": [2, 10]\n    },\n    \"XGBoost\": {\n        \"n_estimators\": [150, 200, 250],\n        \"learning_rate\": [0.05, 0.1, 1],\n        \"max_depth\": [2, 3, 4]\n    }\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:18:37.975332Z","iopub.execute_input":"2025-04-10T19:18:37.975709Z","iopub.status.idle":"2025-04-10T19:18:37.983987Z","shell.execute_reply.started":"2025-04-10T19:18:37.975680Z","shell.execute_reply":"2025-04-10T19:18:37.982723Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"from sklearn.metrics import make_scorer, mean_squared_log_error\nimport numpy as np\n\ndef rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_log_error(y_true, np.maximum(0, y_pred)))\n\nrmsle_scorer = make_scorer(rmsle, greater_is_better=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:18:40.863092Z","iopub.execute_input":"2025-04-10T19:18:40.863489Z","iopub.status.idle":"2025-04-10T19:18:40.868647Z","shell.execute_reply.started":"2025-04-10T19:18:40.863446Z","shell.execute_reply":"2025-04-10T19:18:40.867520Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nimport mlflow\nimport pandas as pd\n\nresults_df = []\n\nfor name, model in models.items():\n    print(f\"\\nTraining {name}...\")\n    \n    grid = GridSearchCV(model, param_grids[name], scoring=rmsle_scorer, cv=5, n_jobs=-1)\n    grid.fit(X_train_processed, y_train)\n\n    best_model = grid.best_estimator_\n    preds = best_model.predict(X_test_processed)\n    rmsle_score = rmsle(y_test, preds)\n\n    residuals = y_test - preds\n\n    # MLflow tracking\n    with mlflow.start_run(run_name=name) as run:\n        mlflow.log_params(grid.best_params_)\n        mlflow.log_metric(\"RMSLE\", rmsle_score)\n\n        # Feature importance\n        if hasattr(best_model, 'feature_importances_'):\n            importances = best_model.feature_importances_\n\n            # Ensure the column names match the transformed features after encoding\n            # If using OneHotEncoder, get the feature names correctly\n            if hasattr(X_train_processed, 'columns'):  # If X_train_processed is a DataFrame\n                feature_names = X_train_processed.columns\n            else:  # If it's a NumPy array, use range indices\n                feature_names = [f\"Feature_{i}\" for i in range(len(importances))]\n\n            # Create the feature importance DataFrame\n            feat_imp_df = pd.DataFrame({\n                'Feature': feature_names,\n                'Importance': importances\n            }).sort_values(by='Importance', ascending=False)\n\n            # Log feature importance\n            feat_imp_csv = f\"{name}_feature_importance.csv\"\n            feat_imp_df.to_csv(feat_imp_csv, index=False)\n            mlflow.log_artifact(feat_imp_csv)\n\n        # Log predictions\n        pred_df = pd.DataFrame({\n            'Actual': y_test,\n            'Predicted': preds,\n            'Residual': residuals\n        })\n        pred_csv = f\"{name}_predictions.csv\"\n        pred_df.to_csv(pred_csv, index=False)\n        mlflow.log_artifact(pred_csv)\n\n        results_df.append({\n            'Model': name,\n            'BestParams': grid.best_params_,\n            'RMSLE': rmsle_score\n        })\n\n        model_name = f\"HousePrice_{name}\"\n        mlflow.sklearn.log_model(best_model, artifact_path=\"model\", registered_model_name=model_name)\n\n        results_df.append({\n            'Model': name,\n            'BestParams': grid.best_params_,\n            'RMSLE': rmsle_score,\n            'MLflowRunID': run.info.run_id\n        })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:24:46.361147Z","iopub.execute_input":"2025-04-10T19:24:46.361719Z","iopub.status.idle":"2025-04-10T19:28:47.981127Z","shell.execute_reply.started":"2025-04-10T19:24:46.361675Z","shell.execute_reply":"2025-04-10T19:28:47.980054Z"}},"outputs":[{"name":"stdout","text":"\nTraining RandomForest...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[31m2025/04/10 19:26:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\nRegistered model 'HousePrice_RandomForest' already exists. Creating a new version of this model...\n2025/04/10 19:26:35 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: HousePrice_RandomForest, version 2\nCreated version '2' of model 'HousePrice_RandomForest'.\n","output_type":"stream"},{"name":"stdout","text":"🏃 View run RandomForest at: https://dagshub.com/ekvirika/HousePrices.mlflow/#/experiments/5/runs/c3e07e871a5b408ebbf79c51a4a583f5\n🧪 View experiment at: https://dagshub.com/ekvirika/HousePrices.mlflow/#/experiments/5\n\nTraining GradientBoosting...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[31m2025/04/10 19:27:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\nSuccessfully registered model 'HousePrice_GradientBoosting'.\n2025/04/10 19:27:54 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: HousePrice_GradientBoosting, version 1\nCreated version '1' of model 'HousePrice_GradientBoosting'.\n","output_type":"stream"},{"name":"stdout","text":"🏃 View run GradientBoosting at: https://dagshub.com/ekvirika/HousePrices.mlflow/#/experiments/5/runs/80a8224d627447ad9535932195a1ec4c\n🧪 View experiment at: https://dagshub.com/ekvirika/HousePrices.mlflow/#/experiments/5\n\nTraining Bagging...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[31m2025/04/10 19:28:03 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\nSuccessfully registered model 'HousePrice_Bagging'.\n2025/04/10 19:28:06 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: HousePrice_Bagging, version 1\nCreated version '1' of model 'HousePrice_Bagging'.\n","output_type":"stream"},{"name":"stdout","text":"🏃 View run Bagging at: https://dagshub.com/ekvirika/HousePrices.mlflow/#/experiments/5/runs/6e2c588e2a9a495daf62797cbd63e3fb\n🧪 View experiment at: https://dagshub.com/ekvirika/HousePrices.mlflow/#/experiments/5\n\nTraining DecisionTree...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[31m2025/04/10 19:28:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\nSuccessfully registered model 'HousePrice_DecisionTree'.\n2025/04/10 19:28:22 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: HousePrice_DecisionTree, version 1\nCreated version '1' of model 'HousePrice_DecisionTree'.\n","output_type":"stream"},{"name":"stdout","text":"🏃 View run DecisionTree at: https://dagshub.com/ekvirika/HousePrices.mlflow/#/experiments/5/runs/d71f08e7b7ee42619009f1af5ad202d3\n🧪 View experiment at: https://dagshub.com/ekvirika/HousePrices.mlflow/#/experiments/5\n\nTraining XGBoost...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[31m2025/04/10 19:28:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\nSuccessfully registered model 'HousePrice_XGBoost'.\n2025/04/10 19:28:47 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: HousePrice_XGBoost, version 1\nCreated version '1' of model 'HousePrice_XGBoost'.\n","output_type":"stream"},{"name":"stdout","text":"🏃 View run XGBoost at: https://dagshub.com/ekvirika/HousePrices.mlflow/#/experiments/5/runs/2fda25842af5499fb0986be19a5dc6d1\n🧪 View experiment at: https://dagshub.com/ekvirika/HousePrices.mlflow/#/experiments/5\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"complete_pipeline = Pipeline([\n    ('pipeline', pipeline),\n    ('model', best_model)\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ntest_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')  # Replace with your actual file path\n\n# If your test data needs ID columns preserved for submission\nid_column = test_data['Id']  # Replace 'id' with your actual ID column name\ntest_features = test_data.drop(['Id'], axis=1)  # Adjust column names as needed\npredictions = complete_pipeline.predict(test_features)\n\nsubmission = pd.DataFrame({\n    'id': id_column,\n    'SalePrice': predictions\n})\n\n# Save to CSV\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}